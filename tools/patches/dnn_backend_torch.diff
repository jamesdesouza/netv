--- /tmp/ffmpeg-clean/libavfilter/dnn/dnn_backend_torch.cpp	2026-01-12 09:20:05.000000000 -0800
+++ /home/jvdillon/ffmpeg_sources/ffmpeg-snapshot/libavfilter/dnn/dnn_backend_torch.cpp	2026-01-12 13:15:54.730276647 -0800
@@ -25,13 +25,15 @@
 
 #include <torch/torch.h>
 #include <torch/script.h>
-#include <thread>
-#include <mutex>
-#include <condition_variable>
-#include <atomic>
+#include <ATen/cuda/CUDAContext.h>
+#include <nppi_color_conversion.h>
+#include <cuda_runtime.h>
 
 extern "C" {
+#include "libavutil/hwcontext.h"
+#include "libavutil/hwcontext_cuda.h"
 #include "dnn_io_proc.h"
+#include <dlfcn.h>
 #include "dnn_backend_common.h"
 #include "libavutil/opt.h"
 #include "libavutil/mem.h"
@@ -46,11 +48,6 @@
     SafeQueue *request_queue;
     Queue *task_queue;
     Queue *lltask_queue;
-    SafeQueue *pending_queue;       ///< requests waiting for inference
-    std::thread *worker_thread;     ///< background worker thread
-    std::mutex *mutex;              ///< mutex for the condition variable
-    std::condition_variable *cond;  ///< condition variable for worker wakeup
-    std::atomic<bool> worker_stop;  ///< signal for thread exit
 } THModel;
 
 typedef struct THInferRequest {
@@ -127,70 +124,27 @@
     if (!model || !*model)
         return;
 
-    th_model = (THModel *)(*model);
-
-    /* 1. Stop and join the worker thread if it exists */
-    if (th_model->worker_thread) {
-        {
-            std::lock_guard<std::mutex> lock(*th_model->mutex);
-            th_model->worker_stop = true;
-        }
-        th_model->cond->notify_all();
-        th_model->worker_thread->join();
-        delete th_model->worker_thread;
-        th_model->worker_thread = NULL;
-    }
-
-    /* 2. Safely delete C++ synchronization objects */
-    if (th_model->mutex) {
-        delete th_model->mutex;
-        th_model->mutex = NULL;
-    }
-    if (th_model->cond) {
-        delete th_model->cond;
-        th_model->cond = NULL;
-    }
-
-    /* 3. Clean up the pending queue */
-    if (th_model->pending_queue) {
-        while (ff_safe_queue_size(th_model->pending_queue) > 0) {
-            THRequestItem *item = (THRequestItem *)ff_safe_queue_pop_front(th_model->pending_queue);
-            destroy_request_item(&item);
-        }
-        ff_safe_queue_destroy(th_model->pending_queue);
-    }
-
-    /* 4. Clean up standard backend queues */
-    if (th_model->request_queue) {
-        while (ff_safe_queue_size(th_model->request_queue) != 0) {
-            THRequestItem *item = (THRequestItem *)ff_safe_queue_pop_front(th_model->request_queue);
-            destroy_request_item(&item);
-        }
-        ff_safe_queue_destroy(th_model->request_queue);
+    th_model = (THModel *) (*model);
+    while (ff_safe_queue_size(th_model->request_queue) != 0) {
+        THRequestItem *item = (THRequestItem *)ff_safe_queue_pop_front(th_model->request_queue);
+        destroy_request_item(&item);
+    }
+    ff_safe_queue_destroy(th_model->request_queue);
+
+    while (ff_queue_size(th_model->lltask_queue) != 0) {
+        LastLevelTaskItem *item = (LastLevelTaskItem *)ff_queue_pop_front(th_model->lltask_queue);
+        av_freep(&item);
+    }
+    ff_queue_destroy(th_model->lltask_queue);
+
+    while (ff_queue_size(th_model->task_queue) != 0) {
+        TaskItem *item = (TaskItem *)ff_queue_pop_front(th_model->task_queue);
+        av_frame_free(&item->in_frame);
+        av_frame_free(&item->out_frame);
+        av_freep(&item);
     }
-
-    if (th_model->lltask_queue) {
-        while (ff_queue_size(th_model->lltask_queue) != 0) {
-            LastLevelTaskItem *item = (LastLevelTaskItem *)ff_queue_pop_front(th_model->lltask_queue);
-            av_freep(&item);
-        }
-        ff_queue_destroy(th_model->lltask_queue);
-    }
-
-    if (th_model->task_queue) {
-        while (ff_queue_size(th_model->task_queue) != 0) {
-            TaskItem *item = (TaskItem *)ff_queue_pop_front(th_model->task_queue);
-            av_frame_free(&item->in_frame);
-            av_frame_free(&item->out_frame);
-            av_freep(&item);
-        }
-        ff_queue_destroy(th_model->task_queue);
-    }
-
-    /* 5. Final model cleanup */
-    if (th_model->jit_model)
-        delete th_model->jit_model;
-
+    ff_queue_destroy(th_model->task_queue);
+    delete th_model->jit_model;
     av_freep(&th_model);
     *model = NULL;
 }
@@ -239,12 +193,115 @@
     channel_idx = dnn_get_channel_idx_by_layout(input.layout);
     input.dims[height_idx] = task->in_frame->height;
     input.dims[width_idx] = task->in_frame->width;
+
+    infer_request->input_tensor = new torch::Tensor();
+    infer_request->output = new torch::Tensor();
+
+    // Check for CUDA input frames (zero-copy path with NPP color conversion)
+    if (task->in_frame->format == AV_PIX_FMT_CUDA && task->in_frame->hw_frames_ctx) {
+        AVHWFramesContext *hw_frames = (AVHWFramesContext *)task->in_frame->hw_frames_ctx->data;
+        int width = task->in_frame->width;
+        int height = task->in_frame->height;
+
+        // Handle NV12 format - NPP-accelerated YUV to RGB conversion
+        if (hw_frames->sw_format == AV_PIX_FMT_NV12) {
+            const Npp8u* src[2] = {(const Npp8u*)task->in_frame->data[0],
+                                   (const Npp8u*)task->in_frame->data[1]};
+            int src_step = task->in_frame->linesize[0];
+            NppiSize roi = {width, height};
+
+            // Allocate RGB buffer on GPU
+            Npp8u* rgb_buf;
+            int rgb_pitch = width * 3;
+            cudaMalloc(&rgb_buf, height * rgb_pitch);
+
+            // Get stream context for NPP (required for newer CUDA versions)
+            NppStreamContext npp_ctx;
+            cudaStream_t stream = at::cuda::getCurrentCUDAStream().stream();
+            npp_ctx.hStream = stream;
+            cudaGetDevice(&npp_ctx.nCudaDeviceId);
+            cudaDeviceGetAttribute(&npp_ctx.nMultiProcessorCount,
+                                   cudaDevAttrMultiProcessorCount,
+                                   npp_ctx.nCudaDeviceId);
+            cudaDeviceGetAttribute(&npp_ctx.nMaxThreadsPerMultiProcessor,
+                                   cudaDevAttrMaxThreadsPerMultiProcessor,
+                                   npp_ctx.nCudaDeviceId);
+            cudaDeviceGetAttribute(&npp_ctx.nMaxThreadsPerBlock,
+                                   cudaDevAttrMaxThreadsPerBlock,
+                                   npp_ctx.nCudaDeviceId);
+            size_t free_mem, total_mem;
+            cudaMemGetInfo(&free_mem, &total_mem);
+            npp_ctx.nSharedMemPerBlock = 0;
+            cudaDeviceGetAttribute((int*)&npp_ctx.nSharedMemPerBlock,
+                                   cudaDevAttrMaxSharedMemoryPerBlock,
+                                   npp_ctx.nCudaDeviceId);
+            npp_ctx.nCudaDevAttrComputeCapabilityMajor = 0;
+            npp_ctx.nCudaDevAttrComputeCapabilityMinor = 0;
+            cudaDeviceGetAttribute(&npp_ctx.nCudaDevAttrComputeCapabilityMajor,
+                                   cudaDevAttrComputeCapabilityMajor,
+                                   npp_ctx.nCudaDeviceId);
+            cudaDeviceGetAttribute(&npp_ctx.nCudaDevAttrComputeCapabilityMinor,
+                                   cudaDevAttrComputeCapabilityMinor,
+                                   npp_ctx.nCudaDeviceId);
+
+            // NV12 -> RGB using NPP (single optimized kernel, context-based API)
+            NppStatus status = nppiNV12ToRGB_8u_P2C3R_Ctx(src, src_step, rgb_buf, rgb_pitch, roi, npp_ctx);
+            if (status != NPP_SUCCESS) {
+                av_log(ctx, AV_LOG_ERROR, "NPP NV12->RGB failed: %d\n", status);
+                cudaFree(rgb_buf);
+                ret = AVERROR_EXTERNAL;
+                goto err;
+            }
+
+            // Wrap as torch tensor and convert to float [0,1]
+            auto options = torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCUDA);
+            torch::Tensor rgb_hwc = torch::from_blob(rgb_buf, {height, width, 3}, {rgb_pitch, 3, 1}, options);
+
+            *infer_request->input_tensor = rgb_hwc.permute({2, 0, 1})  // HWC -> CHW
+                                                  .unsqueeze(0)        // CHW -> NCHW
+                                                  .to(torch::kFloat32)
+                                                  .div(255.0f)
+                                                  .contiguous();
+
+            cudaFree(rgb_buf);
+            av_log(ctx, AV_LOG_DEBUG, "NV12 -> RGB via NPP done\n");
+            return 0;
+        }
+
+        // Handle 0RGB32 format - direct zero-copy
+        if (hw_frames->sw_format == AV_PIX_FMT_0RGB32 ||
+            hw_frames->sw_format == AV_PIX_FMT_0BGR32) {
+            CUdeviceptr cuda_ptr = (CUdeviceptr)task->in_frame->data[0];
+            int linesize = task->in_frame->linesize[0];
+
+            av_log(ctx, AV_LOG_DEBUG, "Zero-copy CUDA 0RGB32 input %dx%d\n", width, height);
+
+            // Create tensor from CUDA memory (XRGB format, 4 bytes per pixel)
+            auto options = torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCUDA);
+            torch::Tensor xrgb_hwc = torch::from_blob((void*)cuda_ptr, {height, width, 4},
+                                                       {linesize, 4, 1}, options);
+
+            // Extract RGB channels (skip X channel), convert to float [0,1]
+            *infer_request->input_tensor = xrgb_hwc.index({"...", torch::indexing::Slice(1, 4)})  // [H,W,3]
+                                                  .permute({2, 0, 1})  // HWC -> CHW
+                                                  .unsqueeze(0)        // CHW -> NCHW
+                                                  .to(torch::kFloat32)
+                                                  .div(255.0f)
+                                                  .contiguous();
+
+            av_log(ctx, AV_LOG_DEBUG, "Zero-copy CUDA input done\n");
+            return 0;
+        }
+
+        av_log(ctx, AV_LOG_WARNING, "Unsupported CUDA sw_format %d, falling back to CPU\n",
+               hw_frames->sw_format);
+    }
+
+    // CPU fallback path
     input.data = av_malloc(input.dims[height_idx] * input.dims[width_idx] *
                            input.dims[channel_idx] * sizeof(float));
     if (!input.data)
         return AVERROR(ENOMEM);
-    infer_request->input_tensor = new torch::Tensor();
-    infer_request->output = new torch::Tensor();
 
     switch (th_model->model.func_type) {
     case DFT_PROCESS_FRAME:
@@ -340,7 +397,132 @@
     switch (th_model->model.func_type) {
     case DFT_PROCESS_FRAME:
         if (task->do_ioproc) {
-            // Post process can only deal with CPU memory.
+            int out_height = outputs.dims[dnn_get_height_idx_by_layout(outputs.layout)];
+            int out_width = outputs.dims[dnn_get_width_idx_by_layout(outputs.layout)];
+
+            // Check for CUDA output frames (zero-copy output path with NPP)
+            if (task->out_frame->hw_frames_ctx && task->out_frame->format == AV_PIX_FMT_CUDA) {
+                AVHWFramesContext *out_hw_frames = (AVHWFramesContext *)task->out_frame->hw_frames_ctx->data;
+
+                // Ensure output tensor is on CUDA
+                if (!output->is_cuda()) {
+                    *output = output->to(torch::kCUDA);
+                }
+
+                // Model output is NCHW float [0,1], convert to uint8 [0,255]
+                torch::Tensor output_rgb = output->squeeze(0)  // NCHW -> CHW [3, H, W]
+                                                  .mul(255.0f)
+                                                  .clamp(0.0f, 255.0f)
+                                                  .to(torch::kUInt8)
+                                                  .permute({1, 2, 0})   // CHW -> HWC
+                                                  .contiguous();
+
+                // Handle NV12 output format - GPU-based RGB to YUV conversion using PyTorch
+                if (out_hw_frames->sw_format == AV_PIX_FMT_NV12) {
+                    CUdeviceptr y_ptr = (CUdeviceptr)task->out_frame->data[0];
+                    CUdeviceptr uv_ptr = (CUdeviceptr)task->out_frame->data[1];
+                    int y_linesize = task->out_frame->linesize[0];
+                    int uv_linesize = task->out_frame->linesize[1];
+
+                    av_log(th_model->ctx, AV_LOG_DEBUG, "Zero-copy RGB->NV12 output %dx%d\n",
+                           out_width, out_height);
+
+                    // RGB to YUV conversion (BT.601) using PyTorch
+                    torch::Tensor rgb_float = output->squeeze(0).mul(255.0f);  // [3, H, W]
+                    torch::Tensor r = rgb_float[0];  // [H, W]
+                    torch::Tensor g = rgb_float[1];
+                    torch::Tensor b = rgb_float[2];
+
+                    // Y = 0.299*R + 0.587*G + 0.114*B
+                    torch::Tensor y = (0.299f * r + 0.587f * g + 0.114f * b)
+                                      .clamp(0.0f, 255.0f).to(torch::kUInt8).contiguous();
+                    // U = -0.169*R - 0.331*G + 0.500*B + 128
+                    torch::Tensor u_full = (-0.169f * r - 0.331f * g + 0.500f * b + 128.0f).clamp(0.0f, 255.0f);
+                    // V = 0.500*R - 0.419*G - 0.081*B + 128
+                    torch::Tensor v_full = (0.500f * r - 0.419f * g - 0.081f * b + 128.0f).clamp(0.0f, 255.0f);
+
+                    // Downsample U and V by 2x using average pooling
+                    torch::Tensor u_down = torch::nn::functional::avg_pool2d(
+                        u_full.unsqueeze(0).unsqueeze(0),
+                        torch::nn::functional::AvgPool2dFuncOptions(2).stride(2)
+                    ).squeeze().to(torch::kUInt8);
+
+                    torch::Tensor v_down = torch::nn::functional::avg_pool2d(
+                        v_full.unsqueeze(0).unsqueeze(0),
+                        torch::nn::functional::AvgPool2dFuncOptions(2).stride(2)
+                    ).squeeze().to(torch::kUInt8);
+
+                    // Interleave U and V for NV12 format
+                    torch::Tensor uv = torch::stack({u_down, v_down}, 2).contiguous();
+
+                    // Copy Y plane
+                    if (y_linesize == out_width) {
+                        cudaMemcpy((void*)y_ptr, y.data_ptr(),
+                                   out_height * out_width, cudaMemcpyDeviceToDevice);
+                    } else {
+                        for (int row = 0; row < out_height; row++) {
+                            cudaMemcpy((void*)(y_ptr + row * y_linesize),
+                                       (char*)y.data_ptr() + row * out_width,
+                                       out_width, cudaMemcpyDeviceToDevice);
+                        }
+                    }
+
+                    // Copy UV plane
+                    if (uv_linesize == out_width) {
+                        cudaMemcpy((void*)uv_ptr, uv.data_ptr(),
+                                   (out_height / 2) * out_width, cudaMemcpyDeviceToDevice);
+                    } else {
+                        for (int row = 0; row < out_height / 2; row++) {
+                            cudaMemcpy((void*)(uv_ptr + row * uv_linesize),
+                                       (char*)uv.data_ptr() + row * out_width,
+                                       out_width, cudaMemcpyDeviceToDevice);
+                        }
+                    }
+
+                    task->out_frame->width = out_width;
+                    task->out_frame->height = out_height;
+                    task->inference_done++;
+                    av_freep(&request->lltask);
+                    goto cleanup;
+                }
+
+                // Handle 0RGB32 output format
+                if (out_hw_frames->sw_format == AV_PIX_FMT_0RGB32 ||
+                    out_hw_frames->sw_format == AV_PIX_FMT_0BGR32) {
+                    CUdeviceptr out_cuda_ptr = (CUdeviceptr)task->out_frame->data[0];
+                    int out_linesize = task->out_frame->linesize[0];
+
+                    av_log(th_model->ctx, AV_LOG_DEBUG, "Zero-copy output to CUDA 0RGB32 frame %dx%d\n",
+                           out_width, out_height);
+
+                    // Pad RGB (3 channels) to XRGB (4 channels) for 0RGB32 format
+                    torch::Tensor padding = torch::zeros({out_height, out_width, 1},
+                        torch::TensorOptions().dtype(torch::kUInt8).device(torch::kCUDA));
+                    torch::Tensor output_xrgb = torch::cat({padding, output_rgb}, 2).contiguous();
+
+                    // Copy to output frame (4 bytes per pixel)
+                    if (out_linesize == out_width * 4) {
+                        cudaMemcpy((void*)out_cuda_ptr, output_xrgb.data_ptr(),
+                                   out_height * out_width * 4, cudaMemcpyDeviceToDevice);
+                    } else {
+                        for (int row = 0; row < out_height; row++) {
+                            cudaMemcpy((void*)(out_cuda_ptr + row * out_linesize),
+                                       (char*)output_xrgb.data_ptr() + row * out_width * 4,
+                                       out_width * 4, cudaMemcpyDeviceToDevice);
+                        }
+                    }
+
+                    task->out_frame->width = out_width;
+                    task->out_frame->height = out_height;
+                    task->inference_done++;
+                    av_freep(&request->lltask);
+                    goto cleanup;
+                }
+
+                av_log(th_model->ctx, AV_LOG_WARNING, "Unsupported CUDA output sw_format, falling back to CPU\n");
+            }
+
+            // CPU fallback path
             if (output->device() != torch::kCPU)
                 *output = output->to(torch::kCPU);
             outputs.scale = 255;
@@ -362,6 +544,7 @@
     task->inference_done++;
     av_freep(&request->lltask);
 err:
+cleanup:
     th_free_request(infer_request);
 
     if (ff_safe_queue_push_back(th_model->request_queue, request) < 0) {
@@ -370,28 +553,6 @@
     }
 }
 
-static void th_worker_thread(THModel *th_model) {
-    while (true) {
-        THRequestItem *request = NULL;
-        {
-            std::unique_lock<std::mutex> lock(*th_model->mutex);
-            th_model->cond->wait(lock, [&]{
-                return th_model->worker_stop || ff_safe_queue_size(th_model->pending_queue) > 0;
-            });
-
-            if (th_model->worker_stop && ff_safe_queue_size(th_model->pending_queue) == 0)
-                break;
-
-            request = (THRequestItem *)ff_safe_queue_pop_front(th_model->pending_queue);
-        }
-
-        if (request) {
-            th_start_inference(request);
-            infer_completion_callback(request);
-        }
-    }
-}
-
 static int execute_model_th(THRequestItem *request, Queue *lltask_queue)
 {
     THModel *th_model = NULL;
@@ -418,12 +579,14 @@
         goto err;
     }
     if (task->async) {
-        std::lock_guard<std::mutex> lock(*th_model->mutex);
-        if (ff_safe_queue_push_back(th_model->pending_queue, request) < 0) {
-            return AVERROR(ENOMEM);
+        avpriv_report_missing_feature(th_model->ctx, "LibTorch async");
+    } else {
+        ret = th_start_inference((void *)(request));
+        if (ret != 0) {
+            goto err;
         }
-        th_model->cond->notify_one();
-        return 0;
+        infer_completion_callback(request);
+        return (task->inference_done == task->inference_todo) ? 0 : DNN_GENERIC_ERROR;
     }
 
 err:
@@ -507,7 +670,27 @@
             av_log(ctx, AV_LOG_ERROR, "No XPU device found\n");
             goto fail;
         }
-        at::detail::getXPUHooks().initXPU();
+        at::detail::getXPUHooks().init();
+    } else if (device.is_cuda()) {
+        // Load libtorch_cuda.so dynamically to enable CUDA support
+        static void *cuda_handle = NULL;
+        if (!cuda_handle) {
+            const char *cuda_lib_paths[] = {
+                "libtorch_cuda.so",
+                "/home/jvdillon/ffmpeg_sources/libtorch/lib/libtorch_cuda.so",
+                NULL
+            };
+            for (int i = 0; cuda_lib_paths[i] && !cuda_handle; i++) {
+                cuda_handle = dlopen(cuda_lib_paths[i], RTLD_NOW | RTLD_GLOBAL);
+            }
+            if (!cuda_handle) {
+                av_log(ctx, AV_LOG_WARNING, "Could not load libtorch_cuda.so: %s\n", dlerror());
+            }
+        }
+        if (!at::cuda::is_available()) {
+            av_log(ctx, AV_LOG_ERROR, "No CUDA device found\n");
+            goto fail;
+        }
     } else if (!device.is_cpu()) {
         av_log(ctx, AV_LOG_ERROR, "Not supported device:\"%s\"\n", device_name);
         goto fail;
@@ -518,7 +701,7 @@
         (*th_model->jit_model) = torch::jit::load(ctx->model_filename);
         th_model->jit_model->to(device);
     } catch (const c10::Error& e) {
-        av_log(ctx, AV_LOG_ERROR, "Failed to load torch model\n");
+        av_log(ctx, AV_LOG_ERROR, "Failed to load torch model: %s\n", e.what());
         goto fail;
     }
 
@@ -556,16 +739,6 @@
         goto fail;
     }
 
-    th_model->pending_queue = ff_safe_queue_create();
-    if (!th_model->pending_queue) {
-        goto fail;
-    }
-
-    th_model->mutex = new std::mutex();
-    th_model->cond = new std::condition_variable();
-    th_model->worker_stop = false;
-    th_model->worker_thread = new std::thread(th_worker_thread, th_model);
-
     model->get_input = &get_input_th;
     model->get_output = &get_output_th;
     model->filter_ctx = filter_ctx;
